{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO description.\n",
    "\n",
    "Author: Spencer M. Richards\n",
    "        Autonomous Systems Lab (ASL), Stanford\n",
    "        (GitHub: spenrich)\n",
    "\"\"\"\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from functools import partial\n",
    "import time\n",
    "import warnings\n",
    "from math import pi, inf\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jax import config\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "class args_class:\n",
    "    def __init__(self, seed, M, pnorm_init, p_freq, meta_epochs, reg_P, reg_k_R, output_dir, use_x64, hdim=32):\n",
    "        self.seed = seed\n",
    "        self.M = M\n",
    "        self.pnorm_init = pnorm_init\n",
    "        self.p_freq = p_freq\n",
    "        self.meta_epochs = meta_epochs\n",
    "        self.reg_P = reg_P\n",
    "        self.reg_k_R = reg_k_R\n",
    "        self.output_dir = output_dir\n",
    "        self.use_x64 = use_x64\n",
    "        self.hdim = hdim\n",
    "\n",
    "seed = 0\n",
    "M = 50\n",
    "pnorm_init = 2.0\n",
    "p_freq = 2000\n",
    "meta_epochs = 1000\n",
    "output_dir = 'test_u_z'\n",
    "reg_P = 2e-3\n",
    "reg_k_R = 2e-3\n",
    "\n",
    "args = args_class(seed, M, pnorm_init, p_freq, meta_epochs, reg_P, reg_k_R, output_dir, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set precision\n",
    "if args.use_x64:\n",
    "    os.environ['JAX_ENABLE_X64'] = 'True'\n",
    "\n",
    "import jax                                          # noqa: E402\n",
    "import jax.numpy as jnp                             # noqa: E402\n",
    "from jax.example_libraries import optimizers             # noqa: E402\n",
    "from dynamics import prior                          # noqa: E402\n",
    "from utils import (tree_normsq, rk38_step, epoch,   # noqa: E402\n",
    "                   odeint_fixed_step, random_ragged_spline, spline,\n",
    "            params_to_cholesky, params_to_posdef, \n",
    "            quaternion_to_rotation_matrix, hat, vee)\n",
    "\n",
    "import jax.debug as jdebug\n",
    "\n",
    "def convert_p_qbar(p):\n",
    "    return jnp.sqrt(1/(1 - 1/p) - 1.1)\n",
    "\n",
    "def convert_qbar_p(qbar):\n",
    "    return 1/(1 - 1/(1.1 + qbar**2))\n",
    "\n",
    "# Initialize PRNG key\n",
    "key = jax.random.PRNGKey(args.seed)\n",
    "\n",
    "# Hyperparameters\n",
    "hparams = {\n",
    "    'seed':        args.seed,     #\n",
    "    'use_x64':     args.use_x64,  #\n",
    "    'num_subtraj': args.M,        # number of trajectories to sub-sample\n",
    "\n",
    "    # For training the model ensemble\n",
    "    'ensemble': {\n",
    "        'num_hlayers':    2,     # number of hidden layers in each model\n",
    "        'hdim':           args.hdim,    # number of hidden units per layer\n",
    "        'train_frac':     0.75,  # fraction of each trajectory for training\n",
    "        'batch_frac':     0.25,  # fraction of training data per batch\n",
    "        'regularizer_l2': 1e-4,  # coefficient for L2-regularization\n",
    "        'learning_rate':  1e-2,  # step size for gradient optimization\n",
    "        'num_epochs':     1000,  # number of epochs\n",
    "        # 'num_epochs':     1,  # number of epochs\n",
    "    },\n",
    "    # For meta-training\n",
    "    'meta': {\n",
    "        'num_hlayers':       2,          # number of hidden layers\n",
    "        'hdim':              args.hdim,         # number of hidden units per layer\n",
    "        'train_frac':        0.75,       #\n",
    "        'learning_rate':     1e-2,       # step size for gradient optimization\n",
    "        'num_steps':         args.meta_epochs,        # maximum number of gradient steps\n",
    "        'regularizer_l2':    1e-4,       # coefficient for L2-regularization\n",
    "        'regularizer_ctrl':  1e-3,       #\n",
    "        'regularizer_error': 0.,         #\n",
    "        'T':                 5.,         # time horizon for each reference\n",
    "        'dt':                1e-2,       # time step for numerical integration\n",
    "        'num_refs':          10,         # reference trajectories to generate\n",
    "        'num_knots':         6,          # knot points per reference spline\n",
    "        'poly_orders':       (9, 9, 9),  # spline orders for each DOF\n",
    "        'deriv_orders':      (4, 4, 4),  # smoothness objective for each DOF\n",
    "        'min_step':          (-2., -2., -0.25),    #\n",
    "        'max_step':          (2., 2., 0.25),       #\n",
    "        'min_ref':           (-4.25, -3.5, 0.0),  #\n",
    "        'max_ref':           (4.5, 4.25, 2.0),     #\n",
    "        'p_freq':            args.p_freq,          # frequency for p-norm update\n",
    "        'regularizer_P':     args.reg_P,           # coefficient for P regularization\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENSEMBLE TRAINING: Pre-compiling ... done (1.18 s)!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8d2d8c4df5420a8bedd2b7d79e01cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DATA PROCESSING ########################################################\n",
    "# Load raw data and arrange in samples of the form\n",
    "# `(t, x, u, t_next, x_next)` for each trajectory, where `x := (q,dq)`\n",
    "with open('data/2024-04-12_00-46-09_traj50_seed0.pkl', 'rb') as file:\n",
    "    raw = pickle.load(file)\n",
    "num_dof = raw['q'].shape[-1]       # number of degrees of freedom\n",
    "param_dim = 2*num_dof + 9 + 3    # number of degrees of freedom including attitude (9 for rotation matrix, 3 for angular velocity)\n",
    "num_traj = raw['q'].shape[0]       # total number of raw trajectories\n",
    "num_samples = raw['t'].size - 1    # number of transitions per trajectory\n",
    "t = jnp.tile(raw['t'][:-1], (num_traj, 1))\n",
    "t_next = jnp.tile(raw['t'][1:], (num_traj, 1))\n",
    "x = jnp.concatenate((raw['q'][:, :-1], raw['dq'][:, :-1]), axis=-1)\n",
    "x_next = jnp.concatenate((raw['q'][:, 1:], raw['dq'][:, 1:]), axis=-1)\n",
    "u = raw['u'][:, :-1, :3]\n",
    "quat = raw['quat'][:, :-1]\n",
    "R = jax.vmap(jax.vmap(quaternion_to_rotation_matrix, in_axes=0), in_axes=0)(quat)\n",
    "R_flatten = R.reshape(R.shape[0], R.shape[1], -1)\n",
    "omega = raw['omega'][:, :-1]\n",
    "data = {'t': t, 'x': x, 'u': u, 'R_flatten': R_flatten, 'omega': omega, 't_next': t_next, 'x_next': x_next}\n",
    "\n",
    "# Shuffle and sub-sample trajectories\n",
    "if hparams['num_subtraj'] > num_traj:\n",
    "    warnings.warn('Cannot sub-sample {:d} trajectories! '\n",
    "                    'Capping at {:d}.'.format(hparams['num_subtraj'],\n",
    "                                            num_traj))\n",
    "    hparams['num_subtraj'] = num_traj\n",
    "\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "shuffled_idx = jax.random.permutation(subkey, num_traj)\n",
    "hparams['subtraj_idx'] = shuffled_idx[:hparams['num_subtraj']]\n",
    "data = jax.tree_util.tree_map(\n",
    "    lambda a: jnp.take(a, hparams['subtraj_idx'], axis=0),\n",
    "    data\n",
    ")\n",
    "\n",
    "# MODEL ENSEMBLE TRAINING ################################################\n",
    "# Loss function along a trajectory\n",
    "def ode(x, R_flatten, omega, t, u, params, prior=prior):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    num_dof = x.size // 2\n",
    "    q, dq = x[:num_dof], x[num_dof:]\n",
    "    H, C, g, B = prior(q, dq)\n",
    "\n",
    "    # Each model in the ensemble is a feed-forward neural network\n",
    "    # with zero output bias\n",
    "    f_ext = x\n",
    "    f_ext = jnp.concatenate([f_ext, R_flatten, omega], axis=0)\n",
    "    for W, b in zip(params['W'], params['b']):\n",
    "        f_ext = jnp.tanh(W@f_ext + b)\n",
    "    f_ext = params['A'] @ f_ext\n",
    "    ddq = jax.scipy.linalg.solve(H, B@u + f_ext - C@dq - g, assume_a='pos')\n",
    "    dx = jnp.concatenate((dq, ddq))\n",
    "    return dx\n",
    "\n",
    "def loss(params, regularizer, t, x, R_flatten, omega, u, t_next, x_next, ode=ode):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    num_samples = t.size\n",
    "    dt = t_next - t\n",
    "    x_next_est = jax.vmap(rk38_step, (None, 0, 0, 0, 0, 0, 0, None))(\n",
    "        ode, dt, x, R_flatten, omega, t, u, params\n",
    "    )\n",
    "    loss = (jnp.sum((x_next_est - x_next)**2)\n",
    "            + regularizer*tree_normsq(params)) / num_samples\n",
    "    return loss\n",
    "\n",
    "# Parallel updates for each model in the ensemble\n",
    "@partial(jax.jit, static_argnums=(4, 5))\n",
    "@partial(jax.vmap, in_axes=(None, 0, None, 0, None, None))\n",
    "def step(idx, opt_state, regularizer, batch, get_params, update_opt,\n",
    "            loss=loss):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    params = get_params(opt_state)\n",
    "    grads = jax.grad(loss, argnums=0)(params, regularizer, **batch)\n",
    "    opt_state = update_opt(idx, grads, opt_state)\n",
    "    return opt_state\n",
    "\n",
    "@jax.jit\n",
    "@jax.vmap\n",
    "def update_best_ensemble(old_params, old_loss, new_params, batch):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    new_loss = loss(new_params, 0., **batch)  # do not regularize\n",
    "    best_params = jax.tree_util.tree_map(\n",
    "        lambda x, y: jnp.where(new_loss < old_loss, x, y),\n",
    "        new_params,\n",
    "        old_params\n",
    "    )\n",
    "    best_loss = jnp.where(new_loss < old_loss, new_loss, old_loss)\n",
    "    return best_params, best_loss, new_loss\n",
    "\n",
    "# Initialize model parameters\n",
    "num_models = hparams['num_subtraj']  # one model per trajectory\n",
    "num_hlayers = hparams['ensemble']['num_hlayers']\n",
    "hdim = hparams['ensemble']['hdim']\n",
    "if num_hlayers >= 1:\n",
    "    shapes = [(hdim, param_dim), ] + (num_hlayers-1)*[(hdim, hdim), ]\n",
    "else:\n",
    "    shapes = []\n",
    "key, *subkeys = jax.random.split(key, 1 + 2*num_hlayers + 1)\n",
    "keys_W = subkeys[:num_hlayers]\n",
    "keys_b = subkeys[num_hlayers:-1]\n",
    "key_A = subkeys[-1]\n",
    "ensemble = {\n",
    "    # hidden layer weights\n",
    "    'W': [0.1*jax.random.normal(keys_W[i], (num_models, *shapes[i]))\n",
    "            for i in range(num_hlayers)],\n",
    "    # hidden layer biases\n",
    "    'b': [0.1*jax.random.normal(keys_b[i], (num_models, shapes[i][0]))\n",
    "            for i in range(num_hlayers)],\n",
    "    # last layer weights\n",
    "    'A': 0.1*jax.random.normal(key_A, (num_models, num_dof, hdim))\n",
    "}\n",
    "\n",
    "# Shuffle samples in time along each trajectory, then split each\n",
    "# trajectory into training and validation sets (i.e., for each model)\n",
    "key, *subkeys = jax.random.split(key, 1 + num_models)\n",
    "subkeys = jnp.asarray(subkeys)\n",
    "shuffled_data = jax.tree_util.tree_map(\n",
    "    lambda a: jax.vmap(jax.random.permutation)(subkeys, a),\n",
    "    data\n",
    ")\n",
    "num_train_samples = int(hparams['ensemble']['train_frac'] * num_samples)\n",
    "ensemble_train_data = jax.tree_util.tree_map(\n",
    "    lambda a: a[:, :num_train_samples],\n",
    "    shuffled_data\n",
    ")\n",
    "ensemble_valid_data = jax.tree_util.tree_map(\n",
    "    lambda a: a[:, num_train_samples:],\n",
    "    shuffled_data\n",
    ")\n",
    "\n",
    "# Initialize gradient-based optimizer (ADAM)\n",
    "learning_rate = hparams['ensemble']['learning_rate']\n",
    "batch_size = int(hparams['ensemble']['batch_frac'] * num_train_samples)\n",
    "num_batches = num_train_samples // batch_size\n",
    "init_opt, update_opt, get_params = optimizers.adam(learning_rate)\n",
    "opt_states = jax.vmap(init_opt)(ensemble)\n",
    "get_ensemble = jax.jit(jax.vmap(get_params))\n",
    "step_idx = 0\n",
    "best_idx = jnp.zeros(num_models)\n",
    "\n",
    "# Pre-compile before training\n",
    "print('ENSEMBLE TRAINING: Pre-compiling ... ', end='', flush=True)\n",
    "start = time.time()\n",
    "batch = next(epoch(key, ensemble_train_data, batch_size,\n",
    "                    batch_axis=1, ragged=False))\n",
    "_ = step(step_idx, opt_states, hparams['ensemble']['regularizer_l2'],\n",
    "            batch, get_params, update_opt)\n",
    "inf_losses = jnp.broadcast_to(jnp.inf, (num_models,))\n",
    "best_ensemble, best_losses, _ = update_best_ensemble(ensemble,\n",
    "                                                        inf_losses,\n",
    "                                                        ensemble,\n",
    "                                                        ensemble_valid_data)\n",
    "_ = get_ensemble(opt_states)\n",
    "end = time.time()\n",
    "print('done ({:.2f} s)!'.format(end - start))\n",
    "\n",
    "ensemble_loss = []\n",
    "# Do gradient descent\n",
    "for _ in tqdm(range(hparams['ensemble']['num_epochs'])):\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    total_loss = 0\n",
    "    for batch in epoch(subkey, ensemble_train_data, batch_size,\n",
    "                        batch_axis=1, ragged=False):\n",
    "        opt_states = step(step_idx, opt_states,\n",
    "                            hparams['ensemble']['regularizer_l2'],\n",
    "                            batch, get_params, update_opt)\n",
    "        new_ensemble = get_ensemble(opt_states)\n",
    "        old_losses = best_losses\n",
    "        best_ensemble, best_losses, valid_losses = update_best_ensemble(\n",
    "            best_ensemble, best_losses, new_ensemble, batch\n",
    "        )\n",
    "        step_idx += 1\n",
    "        best_idx = jnp.where(old_losses == best_losses,\n",
    "                                best_idx, step_idx)\n",
    "        \n",
    "        total_loss += jnp.sum(best_losses)\n",
    "    epoch_avg_loss = total_loss / num_models\n",
    "    ensemble_loss.append(epoch_avg_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# META-TRAINING ##########################################################\n",
    "# k_R = jnp.array([1400.0, 1400.0, 1260.0])/1000.0\n",
    "# k_R = jnp.array([5.0, 5.0, 4.0])\n",
    "# k_R = jnp.array([4.250, 4.250, 0.300])*2\n",
    "# k_R = jnp.array([1.4, 1.4, 1.26])*4\n",
    "# k_Omega = jnp.array([330.0, 330.0, 300.0])/1000.0\n",
    "# k_Omega = jnp.array([0.330, 0.330, 0.300])\n",
    "J = jnp.diag(jnp.array([0.03, 0.03, 0.09]))\n",
    "\n",
    "def ode(z, t, meta_params, pnorm_param, params, reference, prior=prior):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    x, R_flatten, Omega, pA, c = z\n",
    "    num_dof = x.size // 2\n",
    "    q, dq = x[:num_dof], x[num_dof:]\n",
    "    r = reference(t)\n",
    "    dr = jax.jacfwd(reference)(t)\n",
    "    ddr = jax.jacfwd(jax.jacfwd(reference))(t)\n",
    "\n",
    "    # Regressor features\n",
    "    y = x\n",
    "    y = jnp.concatenate([y, R_flatten, Omega], axis=0)\n",
    "    for W, b in zip(meta_params['W'], meta_params['b']):\n",
    "        y = jnp.tanh(W@y + b)\n",
    "\n",
    "    # # Parameterized control and adaptation gains\n",
    "    # gains = jax.tree_util.tree_map(\n",
    "    #     lambda x: params_to_posdef(x),\n",
    "    #     meta_params['gains']\n",
    "    # )\n",
    "\n",
    "    vectorized_keys = {'Λ', 'K', 'P'}\n",
    "    gains = {\n",
    "        key: params_to_posdef(value) if key in vectorized_keys else value\n",
    "        for key, value in meta_params['gains'].items()\n",
    "    }\n",
    "    Λ, K, P, k_R, k_Omega = gains['Λ'], gains['K'], gains['P'], gains['k_R'], gains['k_Omega']\n",
    "\n",
    "    qn = 1.1 + pnorm_param['pnorm']**2\n",
    "\n",
    "    # A = jax.scipy.linalg.sqrtm(P) @ (jnp.maximum(jnp.abs(pA), 1e-6 * jnp.ones_like(pA))**(qn-1) * jnp.sign(pA) * (jnp.ones_like(pA) - jnp.isclose(pA, 0, atol=1e-6)))\n",
    "    # Previous implementation P size: feature_size x feature_size\n",
    "    A = (jnp.maximum(jnp.abs(pA), 1e-6 * jnp.ones_like(pA))**(qn-1) * jnp.sign(pA) * (jnp.ones_like(pA) - jnp.isclose(pA, 0, atol=1e-6))) @ P\n",
    "\n",
    "    # Auxiliary signals\n",
    "    e, de = q - r, dq - dr\n",
    "    v, dv = dr - Λ@e, ddr - Λ@de\n",
    "    s = de + Λ@e\n",
    "\n",
    "    # Controller and adaptation law\n",
    "    H, C, g, B = prior(q, dq)\n",
    "    f_ext_hat = A@y\n",
    "    τ = H@dv + C@v + g - f_ext_hat - K@s\n",
    "    u_d = jnp.linalg.solve(B, τ)\n",
    "    # dA = jax.scipy.linalg.sqrtm(P) @ jnp.outer(s, y)\n",
    "    dA = jnp.outer(s, y) @ P\n",
    "\n",
    "    R = R_flatten.reshape((3,3))\n",
    "\n",
    "    f_d = jnp.linalg.norm(u_d)\n",
    "    b_3d = u_d / jnp.linalg.norm(u_d)\n",
    "    b_1d = jnp.array([1, 0, 0])\n",
    "    cross = jnp.cross(b_3d, b_1d)\n",
    "    b_2d = cross / jnp.linalg.norm(cross)\n",
    "\n",
    "    R_d = jnp.column_stack((jnp.cross(b_2d, b_3d), b_2d, b_3d))\n",
    "\n",
    "    Omega_d = jnp.array([0, 0, 0])\n",
    "    dOmega_d = jnp.array([0, 0, 0])\n",
    "\n",
    "    e_R = 0.5 * vee(R_d.T@R - R.T@R_d)\n",
    "    e_Omega = Omega - R.T@R_d@Omega_d\n",
    "\n",
    "    M = - k_R*e_R \\\n",
    "        - k_Omega*e_Omega \\\n",
    "        + jnp.cross(Omega, J@Omega) \\\n",
    "        - J@(hat(Omega)@R.T@R_d@Omega_d - R.T@R_d@dOmega_d)\n",
    "\n",
    "    dOmega = jax.scipy.linalg.solve(J, M - jnp.cross(Omega, J@Omega), assume_a='pos')\n",
    "    dR = R@hat(Omega)\n",
    "    dR_flatten = dR.flatten()\n",
    "\n",
    "    e_3 = jnp.array([0, 0, 1])\n",
    "    u = f_d*R@e_3\n",
    "\n",
    "    # Apply control to \"true\" dynamics\n",
    "    f_ext = x\n",
    "    f_ext = jnp.concatenate([f_ext, R_flatten, Omega], axis=0)\n",
    "    for W, b in zip(params['W'], params['b']):\n",
    "        f_ext = jnp.tanh(W@f_ext + b)\n",
    "    f_ext = params['A'] @ f_ext\n",
    "    ddq = jax.scipy.linalg.solve(H, u + f_ext - C@dq - g, assume_a='pos')\n",
    "    dx = jnp.concatenate((dq, ddq))\n",
    "\n",
    "    # Estimation loss\n",
    "    # chol_P = params_to_cholesky(meta_params['gains']['P'])\n",
    "    # f_error = f_hat - f\n",
    "    # loss_est = f_error@jax.scipy.linalg.cho_solve((chol_P, True),\n",
    "    #                                               f_error)\n",
    "\n",
    "    # Integrated cost terms\n",
    "    dc = jnp.array([\n",
    "        e@e + de@de,                # tracking loss\n",
    "        u_d@u_d,                        # control loss\n",
    "        (f_ext_hat - f_ext)@(f_ext_hat - f_ext),    # estimation loss\n",
    "    ])\n",
    "\n",
    "    # Assemble derivatives\n",
    "    dz = (dx, dR_flatten, dOmega, dA, dc)\n",
    "    return dz\n",
    "\n",
    "# Simulate adaptive control loop on each model in the ensemble\n",
    "def ensemble_sim(meta_params, pnorm_param, ensemble_params, reference, T, dt, ode=ode):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    # Initial conditions\n",
    "    r0 = reference(0.)\n",
    "    dr0 = jax.jacfwd(reference)(0.)\n",
    "    num_dof = r0.size\n",
    "    num_features = meta_params['W'][-1].shape[0]\n",
    "    x0 = jnp.concatenate((r0, dr0))\n",
    "    # R_flatten0 = jnp.zeros(9)\n",
    "    # R0 = jnp.array(\n",
    "    #     [[ 1.,  0.,  0.],\n",
    "    #     [ 0., -1.,  0.],\n",
    "    #     [ 0.,  0., -1.]]\n",
    "    # )\n",
    "    R0 = jnp.array(\n",
    "        [[ 1.,  0.,  0.],\n",
    "        [ 0., 1.,  0.],\n",
    "        [ 0.,  0., 1.]]\n",
    "    )\n",
    "    R_flatten0 = R0.flatten()\n",
    "    Omega0 = jnp.zeros(3)\n",
    "    A0 = jnp.zeros((num_dof, num_features))\n",
    "    c0 = jnp.zeros(3)\n",
    "    z0 = (x0, R_flatten0, Omega0, A0, c0)\n",
    "\n",
    "    # Integrate the adaptive control loop using the meta-model\n",
    "    # and EACH model in the ensemble along the same reference\n",
    "    in_axes = (None, None, None, None, None, None, None, 0)\n",
    "    ode = partial(ode, reference=reference)\n",
    "    z, t = jax.vmap(odeint_fixed_step, in_axes)(ode, z0, 0., T, dt,\n",
    "                                                meta_params, pnorm_param,\n",
    "                                                ensemble_params)\n",
    "    x, R_flatten, Omega, A, c = z\n",
    "    return t, x, R_flatten, Omega, A, c\n",
    "\n",
    "# Initialize meta-model parameters\n",
    "num_hlayers = hparams['meta']['num_hlayers']\n",
    "hdim = hparams['meta']['hdim']\n",
    "if num_hlayers >= 1:\n",
    "    shapes = [(hdim, param_dim), ] + (num_hlayers-1)*[(hdim, hdim), ]\n",
    "else:\n",
    "    shapes = []\n",
    "key, *subkeys = jax.random.split(key, 1 + 2*num_hlayers + 5)\n",
    "subkeys_W = subkeys[:num_hlayers]\n",
    "subkeys_b = subkeys[num_hlayers:-5]\n",
    "subkeys_gains = subkeys[-5:]\n",
    "meta_params = {\n",
    "    # hidden layer weights\n",
    "    'W': [0.1*jax.random.normal(subkeys_W[i], shapes[i])\n",
    "            for i in range(num_hlayers)],\n",
    "    # hidden layer biases\n",
    "    'b': [0.1*jax.random.normal(subkeys_b[i], (shapes[i][0],))\n",
    "            for i in range(num_hlayers)],\n",
    "    'gains': {  # vectorized control and adaptation gains\n",
    "        'Λ': 0.1*jax.random.normal(subkeys_gains[0],\n",
    "                                    ((num_dof*(num_dof + 1)) // 2,)),\n",
    "        'K': 0.1*jax.random.normal(subkeys_gains[1],\n",
    "                                    ((num_dof*(num_dof + 1)) // 2,)),\n",
    "        'P': 0.1*jax.random.normal(subkeys_gains[2],\n",
    "                                    ((hdim*(hdim + 1)) // 2,)),\n",
    "        # 'k_R': 1.0*jax.random.normal(subkeys_gains[3],\n",
    "        #                             (3,)),\n",
    "        'k_R': jnp.array([1.4, 1.4, 1.26]),\n",
    "        # 'k_Omega': 0.1*jax.random.normal(subkeys_gains[4],\n",
    "        #                             (3,))\n",
    "        'k_Omega': jnp.array([0.330, 0.330, 0.300]),\n",
    "        # 'P': 0.1*jax.random.normal(subkeys_gains[2],\n",
    "                                #    ((num_dof*(num_dof + 1)) // 2,)),\n",
    "    },\n",
    "}\n",
    "# In the bash script, we always specify p-norm desried initial values\n",
    "# Note that the program always uses the q_bar parameter as the p-norm parameterization\n",
    "# However, the printing function should log the final results in p-norm\n",
    "pnorm_param = {'pnorm': convert_p_qbar(args.pnorm_init)}\n",
    "print(\"Initialize pnorm as {:.2f}\".format(convert_qbar_p(pnorm_param['pnorm'])))\n",
    "\n",
    "# Initialize spline coefficients for each reference trajectory\n",
    "num_refs = hparams['meta']['num_refs']\n",
    "key, *subkeys = jax.random.split(key, 1 + num_refs)\n",
    "subkeys = jnp.vstack(subkeys)\n",
    "in_axes = (0, None, None, None, None, None, None, None, None)\n",
    "min_ref = jnp.asarray(hparams['meta']['min_ref'])\n",
    "max_ref = jnp.asarray(hparams['meta']['max_ref'])\n",
    "t_knots, knots, coefs = jax.vmap(random_ragged_spline, in_axes)(\n",
    "    subkeys,\n",
    "    hparams['meta']['T'],\n",
    "    hparams['meta']['num_knots'],\n",
    "    hparams['meta']['poly_orders'],\n",
    "    hparams['meta']['deriv_orders'],\n",
    "    jnp.asarray(hparams['meta']['min_step']),\n",
    "    jnp.asarray(hparams['meta']['max_step']),\n",
    "    0.7*min_ref + jnp.array([0, 0, -1]),\n",
    "    0.7*max_ref + jnp.array([0, 0, -1]),\n",
    ")\n",
    "# x_coefs, y_coefs, θ_coefs = coefs\n",
    "# x_knots, y_knots, θ_knots = knots\n",
    "r_knots = jnp.dstack(knots)\n",
    "\n",
    "# Simulate the adaptive control loop for each model in the ensemble and\n",
    "# each reference trajectory (i.e., spline coefficients)\n",
    "@partial(jax.vmap, in_axes=(None, None, None, 0, 0, None, None))\n",
    "def simulate(meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt, min_ref=min_ref, max_ref=max_ref):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    # Define a reference trajectory in terms of spline coefficients\n",
    "    def reference(t):\n",
    "        r = jnp.array([spline(t, t_knots, c) for c in coefs]) + jnp.array([0, 0, 1])\n",
    "        r = jnp.clip(r, min_ref, max_ref)\n",
    "        return r\n",
    "    t, x, R_flatten, Omega, A, c = ensemble_sim(meta_params, pnorm_param, ensemble_params,\n",
    "                                reference, T, dt)\n",
    "    return t, x, R_flatten, Omega, A, c\n",
    "\n",
    "@partial(jax.jit, static_argnums=(5, 6))\n",
    "def loss(meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt,\n",
    "            regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P, regularizer_k_R):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    # Simulate on each model for each reference trajectory\n",
    "    t, x, R_flatten, Omega, A, c = simulate(meta_params, pnorm_param, ensemble_params, t_knots,\n",
    "                            coefs, T, dt)\n",
    "\n",
    "    # Sum final costs over reference trajectories and ensemble models\n",
    "    # Note `c` has shape (`num_refs`, `num_models`, `T // dt`, 3)\n",
    "    c_final = jnp.sum(c[:, :, -1, :], axis=(0, 1))\n",
    "\n",
    "    # Form a composite loss by weighting the different cost integrals,\n",
    "    # and normalizing by the number of models, number of reference\n",
    "    # trajectories, and time horizon\n",
    "    num_refs = c.shape[0]\n",
    "    num_models = c.shape[1]\n",
    "    normalizer = T * num_refs * num_models\n",
    "    tracking_loss, control_loss, estimation_loss = c_final\n",
    "    reg_P_penalty = jnp.linalg.norm(meta_params['gains']['P'])**2\n",
    "    # reg_P_penalty = jnp.linalg.norm(params_to_posdef(meta_params['gains']['P']))**2\n",
    "    reg_k_R_penalty = jnp.linalg.norm(meta_params['gains']['k_R'])**2\n",
    "    l2_penalty = tree_normsq((meta_params['W'], meta_params['b']))\n",
    "    # regularization on P Frobenius norm shouldn't be normalized\n",
    "    loss = (tracking_loss\n",
    "            + regularizer_ctrl*control_loss\n",
    "            + regularizer_error*estimation_loss\n",
    "            + regularizer_l2*l2_penalty\n",
    "            ) / normalizer + regularizer_P * reg_P_penalty \\\n",
    "            + regularizer_k_R * reg_k_R_penalty\n",
    "\n",
    "    aux = {\n",
    "        # for each model in ensemble\n",
    "        'loss': loss,\n",
    "        'tracking_loss':   jnp.sum(c[:, :, -1, 0], axis=0) / num_refs,\n",
    "        'control_loss':    jnp.sum(c[:, :, -1, 1], axis=0) / num_refs,\n",
    "        'estimation_loss': jnp.sum(c[:, :, -1, 2], axis=0) / num_refs,\n",
    "        'l2_penalty':      l2_penalty,\n",
    "        'reg_P_penalty': reg_P_penalty,\n",
    "        'reg_k_R_penalty': reg_k_R_penalty,\n",
    "        'normalizer': normalizer,\n",
    "        'eigs_Λ':\n",
    "            jnp.diag(params_to_cholesky(meta_params['gains']['Λ']))**2,\n",
    "        'eigs_K':\n",
    "            jnp.diag(params_to_cholesky(meta_params['gains']['K']))**2,\n",
    "        'eigs_P':\n",
    "            jnp.diag(params_to_cholesky(meta_params['gains']['P']))**2,\n",
    "            # jnp.linalg.eigh(P)[0],\n",
    "        'pnorm': pnorm_param['pnorm'], \n",
    "        'x': x[0, 0],\n",
    "        'A': A[0, 0],\n",
    "        'R_flatten': R_flatten[0, 0],\n",
    "        'k_R': meta_params['gains']['k_R'],\n",
    "        'k_Omega': meta_params['gains']['k_Omega'],\n",
    "    }\n",
    "    return loss, aux\n",
    "\n",
    "# Shuffle and split ensemble into training and validation sets\n",
    "train_frac = hparams['meta']['train_frac']\n",
    "num_train_models = int(train_frac * num_models)\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "model_idx = jax.random.permutation(subkey, num_models)\n",
    "train_model_idx = model_idx[:num_train_models]\n",
    "valid_model_idx = model_idx[num_train_models:]\n",
    "train_ensemble = jax.tree_util.tree_map(lambda x: x[train_model_idx],\n",
    "                                        best_ensemble)\n",
    "valid_ensemble = jax.tree_util.tree_map(lambda x: x[valid_model_idx],\n",
    "                                        best_ensemble)\n",
    "\n",
    "# Split reference trajectories into training and validation sets\n",
    "num_train_refs = int(train_frac * num_refs)\n",
    "train_t_knots = jax.tree_util.tree_map(lambda a: a[:num_train_refs],\n",
    "                                        t_knots)\n",
    "train_coefs = jax.tree_util.tree_map(lambda a: a[:num_train_refs], coefs)\n",
    "valid_t_knots = jax.tree_util.tree_map(lambda a: a[num_train_refs:],\n",
    "                                        t_knots)\n",
    "valid_coefs = jax.tree_util.tree_map(lambda a: a[num_train_refs:], coefs)\n",
    "\n",
    "# Initialize gradient-based optimizer (ADAM)\n",
    "learning_rate = hparams['meta']['learning_rate']\n",
    "init_opt, update_opt, get_params = optimizers.adam(learning_rate)\n",
    "# Update meta_params and pnorm_param separately\n",
    "opt_meta = init_opt(meta_params)\n",
    "opt_pnorm = init_opt(pnorm_param)\n",
    "step_meta_idx = 0\n",
    "step_pnorm_idx = 0\n",
    "best_idx_meta = 0\n",
    "best_idx_pnorm = 0\n",
    "best_loss = jnp.inf\n",
    "best_meta_params = meta_params\n",
    "best_pnorm_param = pnorm_param\n",
    "\n",
    "@partial(jax.jit, static_argnums=(6, 7))\n",
    "def step_meta(idx, opt_state, pnorm_param, ensemble_params, t_knots, coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P, regularizer_k_R):\n",
    "    \"\"\"This function only updates the meta_params in an iteration\"\"\"\n",
    "    meta_params = get_params(opt_state)\n",
    "    grads, aux = jax.grad(loss, argnums=0, has_aux=True)(\n",
    "        meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt,\n",
    "        regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P, regularizer_k_R\n",
    "    )\n",
    "    opt_state = update_opt(idx, grads, opt_state)\n",
    "    return opt_state, aux, grads\n",
    "\n",
    "@partial(jax.jit, static_argnums=(6, 7))\n",
    "def step_pnorm(idx, meta_params, opt_state, ensemble_params, t_knots, coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P, regularizer_k_R):\n",
    "    \"\"\"This function only updates the meta_params in an iteration\"\"\"\n",
    "    pnorm_param = get_params(opt_state)\n",
    "    grads, aux = jax.grad(loss, argnums=1, has_aux=True)(\n",
    "        meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt,\n",
    "        regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P, regularizer_k_R\n",
    "    )\n",
    "    opt_state = update_opt(idx, grads, opt_state)\n",
    "    return opt_state, aux, grads\n",
    "\n",
    "# Pre-compile before training\n",
    "print('META-TRAINING: Pre-compiling ... ', end='', flush=True)\n",
    "dt = hparams['meta']['dt']\n",
    "T = hparams['meta']['T']\n",
    "regularizer_l2 = hparams['meta']['regularizer_l2']\n",
    "regularizer_ctrl = hparams['meta']['regularizer_ctrl']\n",
    "regularizer_error = hparams['meta']['regularizer_error']\n",
    "regularizer_P = hparams['meta']['regularizer_P']\n",
    "regularizer_k_R = hparams['meta']['regularizer_k_R']\n",
    "start = time.time()\n",
    "_ = step_meta(0, opt_meta, pnorm_param, train_ensemble, train_t_knots, train_coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P, regularizer_k_R)\n",
    "_ = step_pnorm(0, meta_params, opt_pnorm, train_ensemble, train_t_knots, train_coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P, regularizer_k_R)\n",
    "_ = loss(meta_params, pnorm_param, valid_ensemble, valid_t_knots, valid_coefs, T, dt,\n",
    "            0., 0., 0., 0., 0.)\n",
    "end = time.time()\n",
    "print('done ({:.2f} s)! Now training ...'.format(\n",
    "        end - start))\n",
    "\n",
    "# Record pnorm and training loss history\n",
    "train_lossaux_history = []\n",
    "valid_loss_history = []\n",
    "pnorm_history = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Do gradient descent\n",
    "output_dir = os.path.join('train_results', args.output_dir)\n",
    "# save_freq = 50\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "for i in tqdm(range(hparams['meta']['num_steps'])):\n",
    "    opt_meta, train_aux_meta, grads_meta = step_meta(\n",
    "        step_meta_idx, opt_meta, pnorm_param, train_ensemble, train_t_knots, train_coefs,\n",
    "        T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P, regularizer_k_R\n",
    "    )\n",
    "\n",
    "    # print('loss: ', train_aux_meta['loss'])\n",
    "    # print('tracking_loss: ', train_aux_meta['tracking_loss']/train_aux_meta['normalizer'])\n",
    "    # print('control_loss_norm: ', regularizer_ctrl*train_aux_meta['control_loss']/train_aux_meta['normalizer'])\n",
    "    # print('l2_penalty_norm: ', regularizer_l2*train_aux_meta['l2_penalty']/train_aux_meta['normalizer'])\n",
    "    # print('reg_P_penalty_norm: ', regularizer_P*train_aux_meta['reg_P_penalty'])\n",
    "    # print('reg_k_R_penalty_norm: ', regularizer_k_R*train_aux_meta['reg_k_R_penalty'])\n",
    "    # print('\\n')\n",
    "    # print('k_R:', train_aux_meta['k_R'])\n",
    "    # print('k_Omega:', train_aux_meta['k_Omega'])\n",
    "\n",
    "    # if i%save_freq == 0:\n",
    "    #     output_path = os.path.join(output_dir, f'step_meta_epoch{i}.pkl')\n",
    "    #     with open(output_path, 'wb') as file:\n",
    "    #         pickle.dump(train_aux_meta, file)\n",
    "\n",
    "    new_meta_params = get_params(opt_meta)\n",
    "\n",
    "    # Update p-norm parameter\n",
    "    # The i+1 is to make sure not to update p-norm at step 0\n",
    "    if (i+1) % hparams['meta']['p_freq'] == 0:\n",
    "        opt_pnorm, train_aux_pnorm, grads_pnorm = step_pnorm(\n",
    "            step_pnorm_idx, new_meta_params, opt_pnorm, train_ensemble, train_t_knots, train_coefs,\n",
    "            T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P, regularizer_k_R\n",
    "        )\n",
    "        step_pnorm_idx += 1\n",
    "        # jdebug.print('{grad_pnorm}', grad_pnorm=grads_pnorm)\n",
    "        # jdebug.print('{meta_grad}', meta_grad=grads_meta)\n",
    "        print(\"Update p-norm to {:.2f} at step {:d}\".format(convert_qbar_p(get_params(opt_pnorm)['pnorm']), step_meta_idx))\n",
    "        pnorm_history.append(convert_qbar_p(get_params(opt_pnorm)['pnorm']))\n",
    "        train_lossaux_history.append(train_aux_pnorm)\n",
    "    else:\n",
    "        train_lossaux_history.append(train_aux_meta)\n",
    "\n",
    "    new_pnorm_param = get_params(opt_pnorm)\n",
    "        \n",
    "    valid_loss, valid_aux = loss(\n",
    "        new_meta_params, new_pnorm_param, valid_ensemble, valid_t_knots, valid_coefs,\n",
    "        T, dt, 0., 0., 0., 0., 0.\n",
    "    )\n",
    "    train_loss, train_aux = loss(\n",
    "        new_meta_params, new_pnorm_param, train_ensemble, train_t_knots, train_coefs,\n",
    "        T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P, regularizer_k_R)\n",
    "    \n",
    "    valid_loss_history.append(valid_loss)\n",
    "\n",
    "    # Only update best_meta_params when the loss is decreasing\n",
    "    if valid_loss < best_loss:\n",
    "        best_meta_params = new_meta_params\n",
    "        best_pnorm_param = new_pnorm_param\n",
    "        best_loss = valid_loss\n",
    "        best_idx_meta = step_meta_idx\n",
    "        best_idx_pnorm = step_pnorm_idx\n",
    "    step_meta_idx += 1\n",
    "\n",
    "# Save hyperparameters, ensemble, model, and controller\n",
    "output_name = \"seed={:d}_M={:d}_E={:d}_pinit={:.2f}_pfreq={:.0f}_regP={:.4f}\".format(hparams['seed'], num_models, args.meta_epochs, args.pnorm_init, hparams['meta']['p_freq'], hparams['meta']['regularizer_P'])\n",
    "results = {\n",
    "    'best_step_meta': best_idx_meta,\n",
    "    'best_step_pnorm': best_idx_pnorm,\n",
    "    'hparams': hparams,\n",
    "    'ensemble': best_ensemble,\n",
    "    'model': {\n",
    "        'W': best_meta_params['W'],\n",
    "        'b': best_meta_params['b'],\n",
    "    },\n",
    "    'controller': best_meta_params['gains'],\n",
    "    'pnorm': convert_qbar_p(best_pnorm_param['pnorm']),\n",
    "    'regP': hparams['meta']['regularizer_P'],\n",
    "    'train_lossaux_history': train_lossaux_history,\n",
    "    'valid_loss_history': valid_loss_history,\n",
    "    'pnorm_history': pnorm_history,\n",
    "    'ensemble_loss': ensemble_loss,\n",
    "    't_knots': t_knots,\n",
    "    'coefs': coefs,\n",
    "    'min_ref': min_ref,\n",
    "    'max_ref': max_ref\n",
    "}\n",
    "output_dir = os.path.join('train_results', args.output_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "output_path = os.path.join(output_dir, output_name + '.pkl')\n",
    "with open(output_path, 'wb') as file:\n",
    "    pickle.dump(results, file)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Meta-training completes with p-norm chosen as {:.2f}\".format(results['pnorm']))\n",
    "print('done ({:.2f} s)! Best step index for meta params: {}'.format(end - start, best_idx_meta))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
